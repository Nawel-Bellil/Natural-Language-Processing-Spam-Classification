{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0    3900\n",
       "1    1896\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Morsi Store DZ/mc datathon/nlp email spam/Spam Email raw text for NLP.csv\")\n",
    "df.head()\n",
    "df.tail()\n",
    "df[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Morsi Store\n",
      "[nltk_data]     DZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Morsi Store\n",
      "[nltk_data]     DZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'GGggGG',\n",
       " 'feet',\n",
       " 'it',\n",
       " 'going',\n",
       " 'HTML',\n",
       " 'bads',\n",
       " 'bads',\n",
       " 'random',\n",
       " 'badly']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "test_msg = \"Hey,, GGggGG feet it going? <HTML><bads> bads 'random' badly\"\n",
    "\n",
    "test_msg_tokenized = tokenizer.tokenize(test_msg)\n",
    "test_msg_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'gggggg',\n",
       " 'feet',\n",
       " 'it',\n",
       " 'going',\n",
       " 'html',\n",
       " 'bads',\n",
       " 'bads',\n",
       " 'random',\n",
       " 'badly']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg_lowercased = [t.lower() for t in test_msg_tokenized]\n",
    "test_msg_lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordNetLemmatizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  WordNetLemmatizer\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 4\u001b[0m test_msg_lemmatized_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlemmatize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m test_msg_lowercased] \n\u001b[0;32m      5\u001b[0m test_msg_lemmatized_tokens\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordNetLemmatizer' object is not callable"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "test_message_lemmatized_tokens = [lemmatizer.lemmatize(t) for t in test_msg_lowercased]\n",
    "test_message_lemmatized_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
